# 从脚本开始


在最最开始，我先让AI帮我做一个测试用脚本，这个脚本的能力是让LLM帮我翻译md文件，输入文件名，直接输出同名的_zh.md文件。在这个基础上，再去逐步强化。

遇到的第一个问题是超时。我在Mac mini上安装的模型服务器是ollama，ollama的特点是如果一个模型文件长时间不用，它会从内存中释放，直到下次使用时再加载起来，这样就很容易造成API超时。API超时的另一个原因是单次喂给模型的数据太大，模型想太多，用过R1的朋友应该都体验过。

于是，解法也很简单。既然模型需要长时间加载，我们就先预热一下，发一个hello world让他回答。至于单次太大，则可以给源文件分块提交，比如单次提交不超过2000 token这样。

第二个问题是，模型会胡思乱想，翻译完了还会给我加点儿comments，R1这个问题尤其严重。解法是在提示词里告诉模型，如果是翻译结果，放到translation tag里，如果是想法，放到thoughts结果里，最后大概会是这样：
```xml
<translation>这是一句译文</translation>
<thoughts>这是我的想法</thoughts>
```

这样我们只需要去解析翻译结果里的translation标签就可以了。以及考虑到翻译这件事本身不特别需要推理，我就改成了用qwen2.5:7b。

解决了这部分问题之后，我就想到，既然有thoughts，那么我们可以让下一个模型来做校对工作，哪怕是同一个模型，也可以基于翻译结果去做校对，这样会比最初效果略好一点，就像普通人写完代码，自己再从头看一遍，也能发现不少改进点。这样的话，就要求我们把分块的每个块源文本和目标文本都记录下来，于是我让claude帮我设计了一个中间文件，仍然用xml的模式来记录，而提交校对时会从这个中间文件去获取数据，提交给大模型校对。

校对完成之后，还要生成一个diff文件，最左边是源文件，中间是翻译好的文件，右边是校对内容，这些信息都可以在一个html页面上展现出来。

做完上述工作之后，我心里大概有了底，觉得这个活儿可以完整交给claude来做。